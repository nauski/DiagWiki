# ============================================================
# DiagWiki Configuration
# ============================================================
# Copy this file to .env and customize the values for your setup

# ============================================================
# Application Settings
# ============================================================
NODE_ENV=[development|production]
PORT=[backend server port, default: 8001]
LOG_LEVEL=[logging level: DEBUG|INFO|WARNING|ERROR]

# ============================================================
# LLM Configuration - Claude Code CLI
# ============================================================
# This version uses Claude Code CLI for generation (no API key needed)
# Make sure you have claude-code installed: npm install -g @anthropic-ai/claude-code

# Timeout for Claude Code CLI calls (seconds)
# Longer prompts may need more time
LLM_TIMEOUT=[timeout in seconds, recommended: 300.0]

# ============================================================
# Embedding Configuration - Local (sentence-transformers)
# ============================================================
# Embeddings are computed locally using sentence-transformers
# No GPU required - CPU is sufficient for small models

# Embedding model name (from sentence-transformers)
# Recommended options:
#   all-MiniLM-L6-v2: Fast, 384 dimensions, good quality (default)
#   all-mpnet-base-v2: Better quality, 768 dimensions, slower
#   paraphrase-multilingual-MiniLM-L12-v2: Multilingual support
EMBEDDING_MODEL=[model name, default: all-MiniLM-L6-v2]

# Embedding dimension (must match model output)
# all-MiniLM-L6-v2: 384
# all-mpnet-base-v2: 768
EMBEDDING_DIMENSION=[dimension, default: 384]

# ============================================================
# Text Splitting Configuration
# ============================================================
# How to split documents: "token", "word", or "sentence"
TEXT_SPLIT_BY=[split method: token|word|sentence]

# Size of each chunk in tokens/words
TEXT_CHUNK_SIZE=[chunk size, recommended: 1000]

# Overlap between chunks to maintain context
TEXT_CHUNK_OVERLAP=[overlap size, recommended: 50]

# ============================================================
# Localization
# ============================================================
# Default language for wiki generation
# Supported: en (English), ja (Japanese), zh (Chinese), es (Spanish), kr (Korean), vi (Vietnamese), etc.
DEFAULT_LANGUAGE=[language code, default: en]

# ============================================================
# RAG (Retrieval Augmented Generation) Configuration
# ============================================================
# Maximum characters for RAG context (prevents LLM overflow)
MAX_RAG_CONTEXT_CHARS=[max context chars, recommended: 100000]

# Maximum number of source files to include
MAX_SOURCES=[max source files, recommended: 40]

# Maximum characters per file when reading manual references
MAX_FILE_CHARS=[max chars per file, recommended: 50000]

# Default number of chunks to retrieve
RAG_TOP_K=[number of chunks to retrieve, recommended: 40]

# Higher top_k for section identification (more comprehensive view)
RAG_SECTION_ITERATION_TOP_K=[higher top_k for iterations, recommended: 80]

# Maximum tokens for document chunking
MAX_TOKEN_LIMIT=[token limit for documents, recommended: 8192]

# Maximum tokens for embedding (to prevent overflow)
MAX_EMBEDDING_TOKENS=[max embedding tokens, recommended: 6000]

# Characters to show in source previews
SOURCE_PREVIEW_LENGTH=[preview length in chars, recommended: 600]

# ============================================================
# LLM Generation Parameters
# ============================================================
# Temperature for creative generation (0.0-1.0)
# Higher = more creative, Lower = more focused
DEFAULT_TEMPERATURE=[temperature for generation, range: 0.0-1.0, recommended: 0.7]

# Temperature for focused tasks like title generation
FOCUSED_TEMPERATURE=[temperature for focused tasks, range: 0.0-1.0, recommended: 0.3]

# Context window size - Claude has large context (200k tokens)
LARGE_CONTEXT_WINDOW=[context window size, recommended: 100000]

# ============================================================
# API Configuration
# ============================================================
# Thread pool size for concurrent operations
MAX_WORKERS=[number of worker threads, recommended: 4]
